# =========================
# Global defaults for GRwHS variants and baselines
# =========================
seed: 42
device: "cpu"            # Execution backend: "cpu" (works for all solvers) | "gpu" (NumPyro builds only)
output_dir: "outputs"

standardization:
  X: "unit_variance"     # Feature scaling: "unit_variance" suits probabilistic inference, "unit_l2" matches convex solvers
  y_center: true

# -------------------------
# Data settings
# -------------------------
data:
  type: "synthetic"      # "synthetic": generate data on the fly | "loader": read from disk via data.loader paths
  n: 1000
  p: 200
  G: 20
  group_sizes: "equal"   # "equal" or [g1, g2, ...]; required by group-aware baselines and GRwHS group shrinkage

  # Correlation structure for synthetic design matrices
  correlation:
    type: "block"        # "block" | "ar1" | "cs" (compound symmetry); reused across all model families
    rho: 0.5
    block_size: 10       # Only used when type="block"

  # Signal mechanism (controls the ground-truth regression coefficients)
  signal:
    sparsity: 0.10       # Fraction of non-zero coefficients overall
    strong_frac: 0.05    # Fraction of strong signals among the non-zeros
    group_sparsity: null # If set (e.g. 0.3) only ~30% of groups contain non-zeros; null leaves group sparsity unconstrained
    beta_scale_strong: 2.0
    beta_scale_weak: 0.4
    sign_mix: "random"   # "random" | "positive" | "negative"

  noise_sigma: 1.0
  test_ratio: 0.2
  val_ratio: 0.1

  # Real-data loader settings (used when data.type="loader")
  loader:
    path_X: ""
    path_y: ""
    group_map: ""        # Optional: CSV/JSON mapping from feature name to group id; enables learned group structures

# -------------------------
# Model settings (shared interface)
# -------------------------
model:
  name: "grwhs_svi"      # grwhs_svi | grwhs_gibbs | ridge | lasso | elastic_net | group_lasso | sparse_group_lasso | horseshoe | regularized_horseshoe

  # Regularized Horseshoe slab width (GRwHS + regularized horseshoe)
  c: 1.5

  # Heuristic for the global sparsity scale tau; ignored if model.tau0 is set explicitly
  tau0_heuristic:
    s: 20                # Prior guess for the number of relevant coefficients
    mode: "unit_variance"  # When standardization.X="unit_l2" the code multiplies by sqrt(n)

  # Group shrinkage scale phi_g ~ HalfNormal(eta / sqrt(p_g)) (GRwHS only)
  eta: 0.5
  size_adjust: true      # Scale group-wise penalties by group size

  # Half-Cauchy(scale=s0) prior for sigma (SVI/Gibbs); convex baselines ignore this
  halfcauchy_sigma_scale: 1.0

# -------------------------
# Inference settings
# -------------------------
inference:
  svi:
    steps: 2000
    lr: 1.0e-2
    mc_samples: 3          # Monte Carlo samples for stochastic ELBO terms
    batch_size: 256
    natural_grad: true
    elbo_eval_every: 100
    clip_grad_norm: 10.0
    jitter: 1.0e-8         # Numerical jitter for Cholesky/stability
    seed: 123

  gibbs:
    iters: 4000
    burn_in: 2000
    thin: 2
    adapt_slice: true
    jitter: 1.0e-8
    seed: 123

# -------------------------
# Experiment orchestration
# -------------------------
experiments:
  repeats: 5
  save_posterior: true      # SVI: variational parameters; Gibbs: thinned samples; convex baselines skip this
  diagnostics: true
  metrics: ["mse", "r2", "tpr", "fpr", "auc"]
  threshold:
    type: "magnitude"        # "magnitude" | "topk" | "credible" (if posterior draws/intervals are available)
    value: 0.05              # For type="magnitude": select when |beta_hat| > value; for type="topk": number of coefficients
  runtime_profile: false     # Enable to emit timing / memory diagnostics

# -------------------------
# Logging & IO
# -------------------------
logging:
  level: "INFO"              # "DEBUG" | "INFO" | "WARNING"
  progress: true

checkpointing:
  enable: true
  keep_last: 3
  save_every_steps: 200      # Applies to iterative inference (SVI/Gibbs)

# -------------------------
# Visualization defaults
# -------------------------
viz:
  dpi: 150
  style: "default"
