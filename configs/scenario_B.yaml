# ========================================
# Scenario B – p ≫ n, mixed strong/weak signals (refined)
# ----------------------------------------

defaults: "configs/base.yaml"

seed: 42
device: cpu
output_dir: outputs
task: regression

seeds:
  experiment: 42
  split: 42
  data_generation: 4237321008

standardization:
  X: unit_variance     # Feature-wise unit variance helps scale priors sensibly
  y_center: true       # Center y to stabilize noise scale

data:
  seed: 4237321008
  n: 48
  p: 360
  G: 15
  group_sizes: "variable"
  correlation:
    type: "block"
    rho: 0.45          # Moderate within-block correlation stresses group pooling
    block_size: 24

  # -------- Signal design --------
  signal:
    sparsity: 0.06           # About 22 non-zeros out of 360 features
    strong_frac: 0.50        # Half of non-zeros are strong, clearer separation
    beta_scale_strong: 2.3   # Strong effects more distinguishable
    beta_scale_weak: 0.35    # Weak effects milder to reduce false positives
    group_sparsity: 0.80     # More zero groups for cleaner group-level signals
    sign_mix: "random"       # Random sign pattern for non-zero coefficients

  # Classification knobs are irrelevant for regression, kept for completeness
  classification:
    scale: 1.0
    bias: 0.0
    noise_std: 0.0

  noise_sigma: 1.0
  test_ratio: 0.2
  val_ratio: 0.1
  loader:
    path_X: ""               # Optional paths for external data
    path_y: ""
    group_map: ""

model:
  name: grwhs_gibbs

  # -------- Prior strength (tight but not brittle) --------
  # Single global anchor via s_guess; do NOT set tau0 simultaneously.
  # With sparsity ~0.06, s_guess ≈ 22; we keep a slightly tighter 30 to bias toward sparsity.
  s_guess: 22               # Global shrinkage anchor (expected # of nonzeros)

  eta: 1.7                   # Group prior (smaller = tighter). Helps ϕ mix and avoid drifting.
  c: 1.6                     # Slab width (narrower controls mid-size coefficients, improves generalization)
  size_adjust: true          # Scale priors by group size for fair shrinkage
  s0: 3.1                    # Noise prior scale (not too loose to avoid over-wide intervals)
  iters: 25000               # Ensure builder uses the longer chain length

  # -------- Logistic baseline defaults --------
  # Switch to `task: classification` and override `model.name: logistic_regression`
  # to reuse this scenario for standard logistic regression benchmarking.
  logistic:
    penalty: l2
    C: 1.0
    solver: lbfgs
    max_iter: 1000
    tol: 1.0e-6
    fit_intercept: true
    seed: 42

model_variants:
  classification:
    name: grwhs_gibbs_logistic
    iters: 25000

inference_variants:
  classification:
    gibbs:
      burn_in: 12500
      thin: 1
      slice_w: 8.0
      slice_m: 800
      jitter: 1.0e-05


inference:
  gibbs:
    iters: 25000
    burn_in: 12500            # Half burn-in works well with larger slice steps
    thin: 1                  # Light thinning to trade storage for lower autocorrelation
    adapt_slice: false       # Fix step size; we tune it explicitly below
    jitter: 1.0e-05          # Small numerical jitter to stabilize Gaussian blocks
    seed: 123

    # -------- Slice sampler tuning (bigger steps → shorter ACF) --------
    slice_w: 8.0            # Step width for local scales; larger = farther moves
    slice_m: 800            # Max stepping-out steps; pair with slice_w

  # SVI block retained for completeness; not used in this scenario run
  svi:
    steps: 4000
    lr: 0.01
    mc_samples: 3
    batch_size: 48
    natural_grad: true
    elbo_eval_every: 100
    clip_grad_norm: 10.0
    jitter: 1.0e-06
    seed: 123
    use_hutchinson: true
    hutchinson_samples: 12
    cg_tol: 0.001
    cg_maxiter: 200
    natgrad_damping: 0.001
    coupling_clip: 3.0
    cov_damping: 0.0

experiments:
  repeats: 1                 # Run multiple independent repeats for stability
  save_posterior: true       # Keep posterior draws for diagnostics/plots
  diagnostics: true          # Compute R-hat, ESS, etc.

  metrics:
    regression:
      - RMSE
      - PredictiveLogLikelihood
      - Coverage90
      - IntervalWidth90
      - MeanKappa
      - EffectiveDoF
      - AUC-PR
      - F1
    classification:
      - ClassAccuracy
      - ClassF1
      - ClassAUROC
      - ClassAveragePrecision
      - ClassLogLoss
      - ClassBrier
      - AUC-PR
      - F1
  coverage_level: 0.9
  classification_threshold: 0.5
  runtime_profile: false

logging:
  level: INFO
  progress: true

checkpointing:
  enable: true
  keep_last: 3
  save_every_steps: 200

viz:
  dpi: 150
  style: default
